{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-25T13:40:16.193262Z",
     "iopub.status.busy": "2025-12-25T13:40:16.192931Z",
     "iopub.status.idle": "2025-12-25T13:40:16.203055Z",
     "shell.execute_reply": "2025-12-25T13:40:16.201934Z",
     "shell.execute_reply.started": "2025-12-25T13:40:16.193238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-pre-traitement-sift-bovw-pca/y_labels.npy\n",
      "/kaggle/input/dataset-pre-traitement-sift-bovw-pca/image_names.pkl\n",
      "/kaggle/input/dataset-pre-traitement-sift-bovw-pca/label_names.pkl\n",
      "/kaggle/input/dataset-pre-traitement-sift-bovw-pca/y.npy\n",
      "/kaggle/input/dataset-pre-traitement-sift-bovw-pca/X_pca.npy\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T13:40:16.205443Z",
     "iopub.status.busy": "2025-12-25T13:40:16.205050Z",
     "iopub.status.idle": "2025-12-25T13:40:17.176837Z",
     "shell.execute_reply": "2025-12-25T13:40:17.175618Z",
     "shell.execute_reply.started": "2025-12-25T13:40:16.205418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score, hamming_loss,\n",
    "    classification_report, multilabel_confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T13:40:17.178316Z",
     "iopub.status.busy": "2025-12-25T13:40:17.177846Z",
     "iopub.status.idle": "2025-12-25T13:40:17.381986Z",
     "shell.execute_reply": "2025-12-25T13:40:17.380911Z",
     "shell.execute_reply.started": "2025-12-25T13:40:17.178281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8091, 100) (8091, 495)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1️ Chargement des données\n",
    "# -------------------------------\n",
    "X = np.load(\"/kaggle/input/dataset-pre-traitement-sift-bovw-pca/X_pca.npy\")\n",
    "y = np.load(\"/kaggle/input/dataset-pre-traitement-sift-bovw-pca/y.npy\")\n",
    "\n",
    "with open(\"/kaggle/input/dataset-pre-traitement-sift-bovw-pca/image_names.pkl\", \"rb\") as f:\n",
    "    label_names = pickle.load(f)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T13:40:17.383286Z",
     "iopub.status.busy": "2025-12-25T13:40:17.382986Z",
     "iopub.status.idle": "2025-12-25T13:40:17.456230Z",
     "shell.execute_reply": "2025-12-25T13:40:17.455186Z",
     "shell.execute_reply.started": "2025-12-25T13:40:17.383259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train : (6472, 100) | X_test : (1619, 100)\n",
      " X_train : (6472, 495) | y_test : (1619, 100)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2️ Standardisation\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️ Division Train/Test\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\" X_train : {X_train.shape} | X_test : {X_test.shape}\")\n",
    "print(f\" X_train : {y_train.shape} | y_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T13:40:44.611896Z",
     "iopub.status.busy": "2025-12-25T13:40:44.611575Z",
     "iopub.status.idle": "2025-12-25T13:40:44.618814Z",
     "shell.execute_reply": "2025-12-25T13:40:44.617370Z",
     "shell.execute_reply.started": "2025-12-25T13:40:44.611873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4️ Configuration Knn\n",
    "# -------------------------------\n",
    "base_knn = KNeighborsClassifier(n_jobs=1)\n",
    "knn_model = OneVsRestClassifier(base_knn)\n",
    "\n",
    "param_grid = [{\n",
    "    \"estimator__n_neighbors\": [5, 7],\n",
    "    \"estimator__weights\": [\"distance\"],\n",
    "    \"estimator__metric\": [\"euclidean\"],\n",
    "    \"estimator__p\": [1, 2],\n",
    "    \"estimator__algorithm\": [\"brute\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=8,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=2,\n",
    "    n_jobs=1,\n",
    "    refit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T13:40:58.649503Z",
     "iopub.status.busy": "2025-12-25T13:40:58.649065Z",
     "iopub.status.idle": "2025-12-25T13:50:52.825026Z",
     "shell.execute_reply": "2025-12-25T13:50:52.824213Z",
     "shell.execute_reply.started": "2025-12-25T13:40:58.649422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DÉMARRAGE DE L'ENTRAÎNEMENT GRID SEARCH\n",
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.6s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.4s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  19.0s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.7s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.2s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.2s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.3s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=1, estimator__weights=distance; total time=  18.4s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.3s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.3s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.4s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.1s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.1s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.1s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.1s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=5, estimator__p=2, estimator__weights=distance; total time=  18.2s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.7s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.4s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.5s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.7s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.8s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.7s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.6s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=1, estimator__weights=distance; total time=  18.7s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  18.6s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  18.6s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  18.8s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  19.0s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  18.8s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  19.1s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  18.6s\n",
      "[CV] END estimator__algorithm=brute, estimator__metric=euclidean, estimator__n_neighbors=7, estimator__p=2, estimator__weights=distance; total time=  18.9s\n",
      "Entraînement réussi avec GridSearchCV\n",
      " Entraînement terminé en 594.17 secondes\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5️ Entraînement\n",
    "# -------------------------------\n",
    "print(\"\\n DÉMARRAGE DE L'ENTRAÎNEMENT GRID SEARCH\")\n",
    "start = time.time()\n",
    "try:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    print(\"Entraînement réussi avec GridSearchCV\")\n",
    "except Exception as e:\n",
    "    print(f\" Erreur pendant le GridSearch : {e}\")\n",
    "    print(\" Passage à une configuration simple par défaut.\")\n",
    "    base_knn = KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        weights=\"distance\",\n",
    "        metric=\"euclidean\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    best_knn = OneVsRestClassifier(base_knn)\n",
    "    best_knn.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(f\" Entraînement terminé en {end - start:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T13:52:47.591964Z",
     "iopub.status.busy": "2025-12-25T13:52:47.590949Z",
     "iopub.status.idle": "2025-12-25T13:53:24.508504Z",
     "shell.execute_reply": "2025-12-25T13:53:24.507190Z",
     "shell.execute_reply.started": "2025-12-25T13:52:47.591932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MÉTRIQUES DU MODÈLE KNN :\n",
      "Accuracy             : 0.0006\n",
      "Precision_micro      : 0.2235\n",
      "Recall_micro         : 0.0162\n",
      "F1_macro             : 0.0015\n",
      "F1_micro             : 0.0302\n",
      "Hamming_Loss         : 0.0091\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6️ Évaluation\n",
    "# -------------------------------\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision_micro\": precision_score(y_test, y_pred, average=\"micro\", zero_division=0),\n",
    "    \"Recall_micro\": recall_score(y_test, y_pred, average=\"micro\", zero_division=0),\n",
    "    \"F1_macro\": f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "    \"F1_micro\": f1_score(y_test, y_pred, average=\"micro\", zero_division=0),\n",
    "    \"Hamming_Loss\": hamming_loss(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(\"\\n MÉTRIQUES DU MODÈLE KNN :\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:20s} : {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as SegmentationKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Initialisation du binariseur multilabel avec les noms de labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.classes_ = np.array(label_names)\n",
    "# Fonction pour prédire les labels en appliquant un seuil sur les probabilités\n",
    "def predict_with_threshold(best_knn, X, threshold=0.3):\n",
    "    n_samples = X.shape[0]\n",
    "    n_labels = len(best_knn.estimators_) # Nombre de classifieurs (un par label)\n",
    "    y_proba = np.zeros((n_samples, n_labels))\n",
    "    # Calcul des probabilités pour chaque classifieur\n",
    "    for i, est in enumerate(best_knn.estimators_):\n",
    "        if hasattr(est, 'predict_proba'):\n",
    "            proba = est.predict_proba(X)\n",
    "            # Vérification de la forme pour multiclass ou binaire\n",
    "            if proba.shape[1] == 2:\n",
    "                y_proba[:, i] = proba[:, 1]\n",
    "            else:\n",
    "                y_proba[:, i] = proba[:, 0]\n",
    "    # Application du seuil pour obtenir les prédictions binaires\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    return y_pred, y_proba\n",
    "\n",
    "# Recherche du meilleur seuil basé sur le rappel (recall)\n",
    "thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "best_threshold = 0.3\n",
    "best_recall = 0\n",
    "for t in thresholds:\n",
    "    y_pred_t, _ = predict_with_threshold(best_knn, X_test, threshold=t)\n",
    "    # Calcul du recall moyen sur toutes les images\n",
    "    r = recall_score(y_test, y_pred_t, average='samples', zero_division=0)\n",
    "    if r > best_recall:\n",
    "        best_recall = r\n",
    "        best_threshold = t\n",
    "print(f\"Best threshold: {best_threshold} with recall {best_recall:.4f}\")\n",
    "\n",
    "# Prédictions finales avec le meilleur seuil\n",
    "y_pred, y_proba = predict_with_threshold(best_knn, X_test, threshold=best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer une phrase gabarit\n",
    "\n",
    "def generate_sentence_from_keywords(keywords, proba=None):\n",
    "    # Cas où aucun mot-clé n'est détecté\n",
    "    if len(keywords) == 0:\n",
    "        return \"No keywords detected\"\n",
    "    # Tri des mots-clés par probabilité si fournie\n",
    "    if proba is not None:\n",
    "        keywords = [kw for kw, _ in sorted(zip(keywords, proba), key=lambda x: x[1], reverse=True)]\n",
    "    # Génération de la phrase selon le nombre de mots-clés    \n",
    "    if len(keywords) == 1:\n",
    "        return f\"An image of {keywords[0]}\"\n",
    "    elif len(keywords) == 2:\n",
    "        return f\"An image showing {keywords[0]} and {keywords[1]}\"\n",
    "    elif len(keywords) == 3:\n",
    "        return f\"An image of {keywords[0]} with {keywords[1]} and {keywords[2]}\"\n",
    "    else:\n",
    "        # Si plus de 3 mots-clés, on prend les trois premiers pour la phrase principale\n",
    "        main_kw = ', '.join(keywords[:3])\n",
    "        return f\"An image featuring {main_kw} and {keywords[3]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"/kaggle/input/flickr8k/Images\"\n",
    "# Liste des noms de fichiers d'images valides dans le répertoire\n",
    "valid_image_names = os.listdir(IMAGE_DIR)\n",
    "\n",
    "def predict_and_display_improved(idx, X_test, y_test, y_pred, y_proba):\n",
    "    # Extraction des labels prédits et vrais, ainsi que leurs probabilités\n",
    "    pred_idx = np.where(y_pred[idx] == 1)[0]\n",
    "    pred_labels = mlb.classes_[pred_idx]\n",
    "    pred_probs = y_proba[idx][pred_idx]\n",
    "    \n",
    "    true_idx = np.where(y_test[idx] == 1)[0]\n",
    "    true_labels = mlb.classes_[true_idx]\n",
    "    \n",
    "    # Chargement et préparation de l'image\n",
    "    img_name = valid_image_names[idx]\n",
    "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Cannot load image {img_path}\")\n",
    "        return\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Génération de la légende à partir des mots-clés prédits\n",
    "    caption = generate_sentence_from_keywords(list(pred_labels), pred_probs)\n",
    "    \n",
    "    # Affichage de l'image et des résultats \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(img_name)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    text = f\"PREDICTED KEYWORDS:\\n\"\n",
    "    for kw, p in zip(pred_labels, pred_probs):\n",
    "        text += f\" • {kw} ({p:.2f})\\n\"\n",
    "    text += f\"\\nGENERATED CAPTION:\\n{caption}\\n\"\n",
    "    plt.text(0, 0.5, text, fontsize=11, verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    plt.show()\n",
    "    return pred_labels, caption, true_labels\n",
    "\n",
    "# Affichage des exemples\n",
    "for i in range(min(3, len(X_test))):\n",
    "    predict_and_display_improved(i, X_test, y_test, y_pred, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image_kmeans(image_path, n_segments=5, show_result=True):\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Erreur: impossible de charger {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Convertir BGR -> RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Obtenir les dimensions\n",
    "    h, w, c = image_rgb.shape\n",
    "    \n",
    "    # Reshape: (height, width, 3) -> (height*width, 3)\n",
    "    pixels = image_rgb.reshape(-1, 3)\n",
    "    \n",
    "    # Normaliser les valeurs des pixels [0, 255] -> [0, 1]\n",
    "    pixels_normalized = pixels.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Appliquer K-means\n",
    "    print(f\"Segmentation avec K-means (k={n_segments})...\")\n",
    "    kmeans_seg = SegmentationKMeans(\n",
    "        n_clusters=n_segments,\n",
    "        random_state=42,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    \n",
    "    labels = kmeans_seg.fit_predict(pixels_normalized)\n",
    "    centers = kmeans_seg.cluster_centers_\n",
    "    \n",
    "    # Reconstruire l'image segmentée avec les centres de clusters\n",
    "    segmented_pixels = centers[labels]\n",
    "    segmented_image = (segmented_pixels * 255).reshape(h, w, c).astype(np.uint8)\n",
    "    \n",
    "    # Reshape labels pour avoir la forme de l'image\n",
    "    labels_image = labels.reshape(h, w)\n",
    "    \n",
    "    if show_result:\n",
    "        # Affichage\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Image originale\n",
    "        axes[0].imshow(image_rgb)\n",
    "        axes[0].set_title('Image Originale', fontsize=14, fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Image segmentée\n",
    "        axes[1].imshow(segmented_image)\n",
    "        axes[1].set_title(f'Segmentation K-means (k={n_segments})', fontsize=14, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Labels (régions)\n",
    "        im = axes[2].imshow(labels_image, cmap='tab20')\n",
    "        axes[2].set_title('Régions Segmentées', fontsize=14, fontweight='bold')\n",
    "        axes[2].axis('off')\n",
    "        plt.colorbar(im, ax=axes[2], fraction=0.046)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistiques\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"\\nNombre de segments: {len(unique_labels)}\")\n",
    "        print(\"\\nTaille des segments (en pixels):\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            percentage = (count / len(labels)) * 100\n",
    "            print(f\"  Segment {label}: {count} pixels ({percentage:.2f}%)\")\n",
    "    \n",
    "    return segmented_image, labels_image, centers\n",
    "    \n",
    "seg_img1, labels1, centers1 = segment_image_kmeans(\n",
    "    \"/kaggle/input/flickr8k/Images/1022454332_6af2c1449a.jpg\", \n",
    "    n_segments=5, \n",
    "    show_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_scores(ref, hyp):\n",
    "    # Fonction interne pour générer les n-grams d'une liste de tokens\n",
    "    def ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    scores = []\n",
    "    # Calcul des BLEU-1 à BLEU-4\n",
    "    for n in range(1,5):\n",
    "        ref_ngrams = ngrams(ref, n) # n-grams de la référence\n",
    "        hyp_ngrams = ngrams(hyp, n) # n-grams de la prédiction\n",
    "        if len(hyp_ngrams)==0:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            # Comptage des n-grams communs entre référence et prédiction\n",
    "            overlap = sum(1 for g in hyp_ngrams if g in ref_ngrams)\n",
    "            scores.append(overlap/len(hyp_ngrams))\n",
    "    return scores\n",
    "\n",
    "def calculate_cider_score(ref_kw, hyp_kw):\n",
    "    if len(ref_kw)==0 or len(hyp_kw)==0:\n",
    "        return 0 # Cas où l'un des ensembles est vide\n",
    "    # Compteurs de fréquence des mots    \n",
    "    ref_counter = Counter(ref_kw)\n",
    "    hyp_counter = Counter(hyp_kw)\n",
    "    all_words = set(ref_kw) | set(hyp_kw)\n",
    "    # Création des vecteurs de fréquence pour référence et prédiction\n",
    "    ref_vec = {w: ref_counter.get(w,0) for w in all_words}\n",
    "    hyp_vec = {w: hyp_counter.get(w,0) for w in all_words}\n",
    "    # Calcul du produit scalaire et de la norme pour la similarité cosinus\n",
    "    dot = sum(ref_vec[w]*hyp_vec[w] for w in all_words)\n",
    "    norm = (sum(v*v for v in ref_vec.values())**0.5) * (sum(v*v for v in hyp_vec.values())**0.5)\n",
    "    # Retour du score CIDEr (normalisé sur 10)\n",
    "    return (dot/norm)*10 if norm>0 else 0\n",
    "\n",
    "bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores, cider_scores = [], [], [], [], []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    # Extraction des mots-clés prédits et vrais\n",
    "    pred_idx = np.where(y_pred[i]==1)[0]\n",
    "    pred_kw = list(mlb.classes_[pred_idx])\n",
    "    true_idx = np.where(y_test[i]==1)[0]\n",
    "    true_kw = list(mlb.classes_[true_idx])\n",
    "    # Calcul des scores BLEU\n",
    "    b1, b2, b3, b4 = calculate_bleu_scores(true_kw, pred_kw)\n",
    "    bleu1_scores.append(b1)\n",
    "    bleu2_scores.append(b2)\n",
    "    bleu3_scores.append(b3)\n",
    "    bleu4_scores.append(b4)\n",
    "    # Calcul du score CIDEr\n",
    "    cider_scores.append(calculate_cider_score(true_kw, pred_kw))\n",
    "\n",
    "print(\"\\nCaption Metrics:\")\n",
    "print(f\"BLEU-1: {np.mean(bleu1_scores):.4f}\")\n",
    "print(f\"BLEU-2: {np.mean(bleu2_scores):.4f}\")\n",
    "print(f\"BLEU-3: {np.mean(bleu3_scores):.4f}\")\n",
    "print(f\"BLEU-4: {np.mean(bleu4_scores):.4f}\")\n",
    "print(f\"CIDEr: {np.mean(cider_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9120333,
     "sourceId": 14288553,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
